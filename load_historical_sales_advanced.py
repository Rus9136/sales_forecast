#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
- –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞–º
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
"""

import asyncio
import aiohttp
from datetime import datetime, timedelta
import time
import logging
import json
import os
from typing import Dict, Any, Tuple, List, Optional

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
API_BASE_URL = "http://localhost:8002"
SALES_SYNC_ENDPOINT = f"{API_BASE_URL}/api/sales/sync"
STATS_ENDPOINT = f"{API_BASE_URL}/api/sales/stats"
PROGRESS_FILE = "load_progress.json"

# Date configuration
START_DATE = datetime(2024, 6, 1)
END_DATE = datetime(2024, 12, 31)
CHUNK_SIZE_DAYS = 15
DELAY_BETWEEN_REQUESTS = 5
MAX_RETRIES = 3
RETRY_DELAY = 10


class ProgressTracker:
    """Track loading progress"""
    
    def __init__(self, filename: str = PROGRESS_FILE):
        self.filename = filename
        self.progress = self.load_progress()
    
    def load_progress(self) -> Dict[str, Any]:
        """Load progress from file"""
        if os.path.exists(self.filename):
            try:
                with open(self.filename, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {
            'completed_chunks': [],
            'failed_chunks': [],
            'statistics': {
                'total_summary': 0,
                'total_hourly': 0,
                'start_time': None,
                'last_update': None
            }
        }
    
    def save_progress(self):
        """Save progress to file"""
        self.progress['statistics']['last_update'] = datetime.now().isoformat()
        with open(self.filename, 'w') as f:
            json.dump(self.progress, f, indent=2)
    
    def mark_completed(self, from_date: str, to_date: str, records: Dict[str, int]):
        """Mark chunk as completed"""
        chunk_id = f"{from_date}_{to_date}"
        if chunk_id not in self.progress['completed_chunks']:
            self.progress['completed_chunks'].append(chunk_id)
        
        # Remove from failed if it was there
        if chunk_id in self.progress['failed_chunks']:
            self.progress['failed_chunks'].remove(chunk_id)
        
        # Update statistics
        self.progress['statistics']['total_summary'] += records.get('summary', 0)
        self.progress['statistics']['total_hourly'] += records.get('hourly', 0)
        
        self.save_progress()
    
    def mark_failed(self, from_date: str, to_date: str):
        """Mark chunk as failed"""
        chunk_id = f"{from_date}_{to_date}"
        if chunk_id not in self.progress['failed_chunks']:
            self.progress['failed_chunks'].append(chunk_id)
        self.save_progress()
    
    def is_completed(self, from_date: str, to_date: str) -> bool:
        """Check if chunk is already completed"""
        chunk_id = f"{from_date}_{to_date}"
        return chunk_id in self.progress['completed_chunks']


async def sync_sales_with_retry(
    session: aiohttp.ClientSession, 
    from_date: datetime, 
    to_date: datetime,
    max_retries: int = MAX_RETRIES
) -> Dict[str, Any]:
    """Sync sales data with automatic retry on failure"""
    
    from_str = from_date.strftime('%Y-%m-%d')
    to_str = to_date.strftime('%Y-%m-%d')
    
    for attempt in range(max_retries):
        try:
            params = {'from_date': from_str, 'to_date': to_str}
            
            logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}: –ó–∞–≥—Ä—É–∑–∫–∞ {from_str} - {to_str}")
            
            async with session.post(SALES_SYNC_ENDPOINT, params=params, timeout=300) as response:
                result = await response.json()
                
                if result.get('status') == 'success':
                    logger.info(
                        f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {result.get('summary_records', 0)} –¥–Ω–µ–≤–Ω—ã—Ö, "
                        f"{result.get('hourly_records', 0)} –ø–æ—á–∞—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π"
                    )
                    return result
                else:
                    error_msg = result.get('message', 'Unknown error')
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {error_msg}")
                    
                    # If it's a data issue, no point retrying
                    if 'No data found' in error_msg:
                        return result
                    
                    # Otherwise, retry
                    if attempt < max_retries - 1:
                        logger.info(f"‚è≥ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥...")
                        await asyncio.sleep(RETRY_DELAY)
                        
        except asyncio.TimeoutError:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {from_str} - {to_str}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        
        if attempt < max_retries - 1:
            await asyncio.sleep(RETRY_DELAY)
    
    # All retries failed
    return {
        'status': 'error',
        'message': f'Failed after {max_retries} attempts',
        'summary_records': 0,
        'hourly_records': 0
    }


async def get_current_stats(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """Get current database statistics"""
    try:
        async with session.get(STATS_ENDPOINT) as response:
            if response.status == 200:
                return await response.json()
    except:
        pass
    return {}


async def load_historical_data_advanced():
    """Advanced historical data loading with progress tracking"""
    
    logger.info(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å {START_DATE.date()} –ø–æ {END_DATE.date()}")
    
    # Initialize progress tracker
    tracker = ProgressTracker()
    
    # Generate date chunks
    date_chunks = []
    current_start = START_DATE
    while current_start <= END_DATE:
        current_end = min(current_start + timedelta(days=CHUNK_SIZE_DAYS - 1), END_DATE)
        date_chunks.append((current_start, current_end))
        current_start = current_end + timedelta(days=1)
    
    logger.info(f"üìÖ –í—Å–µ–≥–æ —á–∞—Å—Ç–µ–π: {len(date_chunks)}")
    
    # Check progress
    if tracker.progress['completed_chunks']:
        logger.info(f"üìà –£–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(tracker.progress['completed_chunks'])} —á–∞—Å—Ç–µ–π")
        logger.info(f"üìä –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {tracker.progress['statistics']['total_summary']} –¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    # Start loading
    start_time = time.time()
    if not tracker.progress['statistics']['start_time']:
        tracker.progress['statistics']['start_time'] = datetime.now().isoformat()
    
    async with aiohttp.ClientSession() as session:
        # Get initial stats
        initial_stats = await get_current_stats(session)
        logger.info(f"üìä –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ë–î: {initial_stats.get('summary_records', 0)} –∑–∞–ø–∏—Å–µ–π")
        
        # Process chunks
        for i, (from_date, to_date) in enumerate(date_chunks, 1):
            from_str = from_date.strftime('%Y-%m-%d')
            to_str = to_date.strftime('%Y-%m-%d')
            
            # Skip if already completed
            if tracker.is_completed(from_str, to_str):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ {from_str} - {to_str} (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ)")
                continue
            
            logger.info(f"\n{'='*60}")
            logger.info(f"üì¶ –ß–∞—Å—Ç—å {i}/{len(date_chunks)}")
            
            # Sync with retry
            result = await sync_sales_with_retry(session, from_date, to_date)
            
            # Update progress
            if result.get('status') == 'success':
                tracker.mark_completed(from_str, to_str, {
                    'summary': result.get('summary_records', 0),
                    'hourly': result.get('hourly_records', 0)
                })
            else:
                tracker.mark_failed(from_str, to_str)
            
            # Show progress
            completed = len(tracker.progress['completed_chunks'])
            failed = len(tracker.progress['failed_chunks'])
            remaining = len(date_chunks) - completed - failed
            
            progress_pct = (completed / len(date_chunks)) * 100
            logger.info(
                f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_pct:.1f}% "
                f"(‚úÖ {completed} | ‚ùå {failed} | ‚è≥ {remaining})"
            )
            
            # Delay between requests
            if i < len(date_chunks) and remaining > 0:
                await asyncio.sleep(DELAY_BETWEEN_REQUESTS)
        
        # Get final stats
        final_stats = await get_current_stats(session)
        new_records = final_stats.get('summary_records', 0) - initial_stats.get('summary_records', 0)
    
    # Final report
    elapsed_time = time.time() - start_time
    
    logger.info(f"\n{'='*60}")
    logger.info("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢:")
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(tracker.progress['completed_chunks'])} —á–∞—Å—Ç–µ–π")
    logger.info(f"‚ùå –° –æ—à–∏–±–∫–∞–º–∏: {len(tracker.progress['failed_chunks'])} —á–∞—Å—Ç–µ–π")
    logger.info(f"üìà –í—Å–µ–≥–æ –¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {tracker.progress['statistics']['total_summary']}")
    logger.info(f"üìä –í—Å–µ–≥–æ –ø–æ—á–∞—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {tracker.progress['statistics']['total_hourly']}")
    logger.info(f"üÜï –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {new_records}")
    
    if tracker.progress['failed_chunks']:
        logger.warning("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –ø–µ—Ä–∏–æ–¥—ã:")
        for chunk in tracker.progress['failed_chunks']:
            logger.warning(f"  - {chunk.replace('_', ' –ø–æ ')}")
        logger.info("\nüí° –°–æ–≤–µ—Ç: –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤")
    else:
        logger.info("\nüéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        
        # Offer to clean up progress file
        if os.path.exists(PROGRESS_FILE):
            cleanup = input("\n‚ùì –£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞? (y/n): ").strip().lower()
            if cleanup == 'y':
                os.remove(PROGRESS_FILE)
                logger.info("üóëÔ∏è  –§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —É–¥–∞–ª–µ–Ω")


async def main():
    """Main entry point"""
    logger.info("üîß Sales Forecast Historical Data Loader (Advanced)")
    logger.info(f"üìç API URL: {API_BASE_URL}")
    
    # Test connection
    try:
        async with aiohttp.ClientSession() as session:
            stats = await get_current_stats(session)
            if stats:
                logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å API —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                logger.info(f"üìä –¢–µ–∫—É—â–∞—è –ë–î: {stats.get('summary_records', 0)} –¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
            else:
                raise Exception("API –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {str(e)}")
        logger.error("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –Ω–∞ –ø–æ—Ä—Ç—É 8002")
        return
    
    # Check for existing progress
    if os.path.exists(PROGRESS_FILE):
        logger.info(f"\nüìÇ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞")
        resume = input("‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏? (y/n): ").strip().lower()
        if resume != 'y':
            os.remove(PROGRESS_FILE)
            logger.info("üóëÔ∏è  –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–±—Ä–æ—à–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ")
    
    # Confirmation
    print(f"\n‚ö†Ô∏è  –ë—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å {START_DATE.date()} –ø–æ {END_DATE.date()}")
    print(f"   –†–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏: {CHUNK_SIZE_DAYS} –¥–Ω–µ–π")
    print(f"   –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {DELAY_BETWEEN_REQUESTS} —Å–µ–∫")
    
    confirmation = input("\n‚ùì –ù–∞—á–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É? (y/n): ").strip().lower()
    if confirmation != 'y':
        logger.info("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
        return
    
    # Start loading
    await load_historical_data_advanced()


if __name__ == "__main__":
    asyncio.run(main())