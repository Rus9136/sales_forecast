#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂ –∏–∑ iiko API
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å 01.06.2024 –ø–æ 31.12.2024 —á–∞—Å—Ç—è–º–∏ –ø–æ 15 –¥–Ω–µ–π
"""

import asyncio
import aiohttp
from datetime import datetime, timedelta
import time
import logging
from typing import Dict, Any, Tuple, List

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API configuration
API_BASE_URL = "http://localhost:8002"
SALES_SYNC_ENDPOINT = f"{API_BASE_URL}/api/sales/sync"

# Date configuration
START_DATE = datetime(2024, 6, 1)  # 01.06.2024
END_DATE = datetime(2024, 12, 31)  # 31.12.2024
CHUNK_SIZE_DAYS = 15  # Load 15 days at a time
DELAY_BETWEEN_REQUESTS = 5  # Seconds to wait between requests


def generate_date_chunks(start: datetime, end: datetime, chunk_days: int) -> List[Tuple[datetime, datetime]]:
    """Generate list of date ranges for chunked loading"""
    chunks = []
    current_start = start
    
    while current_start <= end:
        current_end = min(current_start + timedelta(days=chunk_days - 1), end)
        chunks.append((current_start, current_end))
        current_start = current_end + timedelta(days=1)
    
    return chunks


async def sync_sales_chunk(session: aiohttp.ClientSession, from_date: datetime, to_date: datetime) -> Dict[str, Any]:
    """Sync sales data for a specific date range"""
    params = {
        'from_date': from_date.strftime('%Y-%m-%d'),
        'to_date': to_date.strftime('%Y-%m-%d')
    }
    
    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {params['from_date']} - {params['to_date']}")
    
    try:
        async with session.post(SALES_SYNC_ENDPOINT, params=params) as response:
            result = await response.json()
            
            if result.get('status') == 'success':
                logger.info(
                    f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {result.get('summary_records', 0)} –¥–Ω–µ–≤–Ω—ã—Ö, "
                    f"{result.get('hourly_records', 0)} –ø–æ—á–∞—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π"
                )
            else:
                logger.error(
                    f"‚ùå –û—à–∏–±–∫–∞: {result.get('message', 'Unknown error')}\n"
                    f"   –î–µ—Ç–∞–ª–∏: {result.get('details', 'No details')}"
                )
            
            return result
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {from_date} - {to_date}: {str(e)}")
        return {
            'status': 'error',
            'message': str(e),
            'summary_records': 0,
            'hourly_records': 0
        }


async def load_historical_data():
    """Main function to load all historical data"""
    logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å {START_DATE.date()} –ø–æ {END_DATE.date()}")
    
    # Generate date chunks
    date_chunks = generate_date_chunks(START_DATE, END_DATE, CHUNK_SIZE_DAYS)
    logger.info(f"üìÖ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(date_chunks)} —á–∞—Å—Ç–µ–π –ø–æ {CHUNK_SIZE_DAYS} –¥–Ω–µ–π")
    
    # Statistics
    total_summary = 0
    total_hourly = 0
    success_count = 0
    error_count = 0
    
    async with aiohttp.ClientSession() as session:
        for i, (from_date, to_date) in enumerate(date_chunks, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"üì¶ –ß–∞—Å—Ç—å {i}/{len(date_chunks)}")
            
            # Sync data
            result = await sync_sales_chunk(session, from_date, to_date)
            
            # Update statistics
            if result.get('status') == 'success':
                success_count += 1
                total_summary += result.get('summary_records', 0)
                total_hourly += result.get('hourly_records', 0)
            else:
                error_count += 1
            
            # Delay between requests (except for the last one)
            if i < len(date_chunks):
                logger.info(f"‚è≥ –ü–∞—É–∑–∞ {DELAY_BETWEEN_REQUESTS} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º...")
                await asyncio.sleep(DELAY_BETWEEN_REQUESTS)
    
    # Final report
    logger.info(f"\n{'='*60}")
    logger.info("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢:")
    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count} —á–∞—Å—Ç–µ–π")
    logger.info(f"‚ùå –° –æ—à–∏–±–∫–∞–º–∏: {error_count} —á–∞—Å—Ç–µ–π")
    logger.info(f"üìà –í—Å–µ–≥–æ –¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {total_summary}")
    logger.info(f"üìä –í—Å–µ–≥–æ –ø–æ—á–∞—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {total_hourly}")
    logger.info(f"{'='*60}\n")
    
    if error_count > 0:
        logger.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–∏ –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.")
    else:
        logger.info("üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")


async def test_connection():
    """Test connection to the API"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{API_BASE_URL}/api/sales/stats") as response:
                if response.status == 200:
                    stats = await response.json()
                    logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å API —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                    logger.info(f"üìä –¢–µ–∫—É—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats.get('summary_records', 0)} –∑–∞–ø–∏—Å–µ–π –≤ –ë–î")
                    return True
                else:
                    logger.error(f"‚ùå API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å: {response.status}")
                    return False
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API: {str(e)}")
        return False


async def main():
    """Main entry point"""
    logger.info("üîß Sales Forecast Historical Data Loader")
    logger.info(f"üìç API URL: {API_BASE_URL}")
    
    # Test connection first
    if not await test_connection():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –Ω–∞ –ø–æ—Ä—Ç—É 8002")
        return
    
    # Ask for confirmation
    print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å {START_DATE.date()} –ø–æ {END_DATE.date()}")
    print(f"   –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ {len(generate_date_chunks(START_DATE, END_DATE, CHUNK_SIZE_DAYS)) * (DELAY_BETWEEN_REQUESTS + 10) // 60} –º–∏–Ω—É—Ç")
    
    confirmation = input("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/n): ").strip().lower()
    if confirmation != 'y':
        logger.info("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return
    
    # Start loading
    start_time = time.time()
    await load_historical_data()
    
    elapsed_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time/60:.1f} –º–∏–Ω—É—Ç")


if __name__ == "__main__":
    asyncio.run(main())